{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My UNET returns a blank image after traning ðŸ˜”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "def test():\n",
    "    x = torch.randn((3, 1, 161, 161))\n",
    "    model = UNET(in_channels=1, out_channels=1)\n",
    "    preds = model(x)\n",
    "    assert preds.shape == x.shape\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\XPS/data/pascal_voc\\VOCtrainval_11-May-2012.tar\n",
      "Extracting C:\\Users\\XPS/data/pascal_voc\\VOCtrainval_11-May-2012.tar to C:\\Users\\XPS/data/pascal_voc\n",
      "Using downloaded and verified file: C:\\Users\\XPS/data/pascal_voc\\VOCtrainval_11-May-2012.tar\n",
      "Extracting C:\\Users\\XPS/data/pascal_voc\\VOCtrainval_11-May-2012.tar to C:\\Users\\XPS/data/pascal_voc\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "VOC_CLASSES = [\n",
    "    \"background\",\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"potted plant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tv/monitor\",\n",
    "]\n",
    "\n",
    "\n",
    "VOC_COLORMAP = [\n",
    "    [0, 0, 0],\n",
    "    [128, 0, 0],\n",
    "    [0, 128, 0],\n",
    "    [128, 128, 0],\n",
    "    [0, 0, 128],\n",
    "    [128, 0, 128],\n",
    "    [0, 128, 128],\n",
    "    [128, 128, 128],\n",
    "    [64, 0, 0],\n",
    "    [192, 0, 0],\n",
    "    [64, 128, 0],\n",
    "    [192, 128, 0],\n",
    "    [64, 0, 128],\n",
    "    [192, 0, 128],\n",
    "    [64, 128, 128],\n",
    "    [192, 128, 128],\n",
    "    [0, 64, 0],\n",
    "    [128, 64, 0],\n",
    "    [0, 192, 0],\n",
    "    [128, 192, 0],\n",
    "    [0, 64, 128],\n",
    "]\n",
    "\n",
    "\n",
    "class PascalVOCSearchDataset(VOCSegmentation):\n",
    "    def __init__(self, image_set, root=\"~/data/pascal_voc\", download=True, transform=None):\n",
    "        super().__init__(root=root, image_set=image_set, download=download, transform=transform)\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_to_segmentation_mask(mask):\n",
    "        # This function converts a mask from the Pascal VOC format to the format required by AutoAlbument.\n",
    "        #\n",
    "        # Pascal VOC uses an RGB image to encode the segmentation mask for that image. RGB values of a pixel\n",
    "        # encode the pixel's class.\n",
    "        #\n",
    "        # AutoAlbument requires a segmentation mask to be a NumPy array with the shape [height, width, num_classes].\n",
    "        # Each channel in this mask should encode values for a single class. Pixel in a mask channel should have\n",
    "        # a value of 1.0 if the pixel of the image belongs to this class and 0.0 otherwise.\n",
    "        height, width = mask.shape[:2]\n",
    "        segmentation_mask = np.zeros((height, width, len(VOC_COLORMAP)), dtype=np.float32)\n",
    "        for label_index, label in enumerate(VOC_COLORMAP):\n",
    "            segmentation_mask[:, :, label_index] = np.all(mask == label, axis=-1).astype(float)\n",
    "        return segmentation_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.images[index])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks[index])\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = self._convert_to_segmentation_mask(mask)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        return image, mask\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),   # Convert to tensor\n",
    "    transforms.Resize(size=(64,64), antialias=True)\n",
    "])\n",
    "\n",
    "train_set = PascalVOCSearchDataset(image_set=\"train\", transform=transform)\n",
    "val_set = PascalVOCSearchDataset(image_set=\"val\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=32, shuffle=True),\n",
    "    'val': DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNET(in_channels=3, out_channels=21).to(device)\n",
    "\n",
    "initialize_weights(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_writer = SummaryWriter('alaruns/train')\n",
    "val_writer = SummaryWriter('alaruns/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with epoch 0\n",
      "Done with epoch 1\n",
      "Done with epoch 2\n",
      "Done with epoch 3\n",
      "Done with epoch 4\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_batch_idx = 0\n",
    "val_batch_idx = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train the model for one epoch\n",
    "    for i, data in enumerate(dataloaders['train']):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update training loss\n",
    "        train_writer.add_scalar('train_loss', loss.item(), train_batch_idx)\n",
    "        train_batch_idx += 1\n",
    "        train_writer.close()\n",
    "    # print('Done with training')\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    for i, data in enumerate(dataloaders['val']):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update validation loss\n",
    "            val_writer.add_scalar('val_loss', loss.item(), val_batch_idx)\n",
    "            val_writer.close()\n",
    "\n",
    "\n",
    "        val_batch_idx += 1\n",
    "    \n",
    "      \n",
    "    print(f'Done with epoch {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "torch.save(model, 'models/almodel_two.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model weights\n",
    "torch.save(model.state_dict(), 'models/almodel_weights_two.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
